{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 설치 실패시, Internet On/Off 확인\n!pip install easydict","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:34:45.901255Z","iopub.execute_input":"2021-06-07T13:34:45.901968Z","iopub.status.idle":"2021-06-07T13:34:56.594777Z","shell.execute_reply.started":"2021-06-07T13:34:45.901851Z","shell.execute_reply":"2021-06-07T13:34:56.593786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, random, pickle\nfrom os.path import join\nfrom glob import glob\nfrom time import time\n\nfrom tqdm.notebook import tqdm\n\nfrom easydict import EasyDict as edict\n\nimport numpy as np\nfrom PIL import Image\n\nfrom sklearn.svm import OneClassSVM\n\nfrom sklearn.decomposition import PCA, KernelPCA\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML, display\n","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:34:56.596676Z","iopub.execute_input":"2021-06-07T13:34:56.597096Z","iopub.status.idle":"2021-06-07T13:34:57.773416Z","shell.execute_reply.started":"2021-06-07T13:34:56.597049Z","shell.execute_reply":"2021-06-07T13:34:57.77229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### DATA SET\n\ntrain : 3629장 -> normal.<br>\ntest : 467(good) + 1258(fault case) -> normal(good) and abnormal(failt case). <br>\nall : 5354장 <br>\n\n\n{ClassName} Original dataset과 동일 <br>\n|--val.csv : col1 : ID. test 영상 파일명, col2 : label. good:1, fault:-1 <br>\n|--train <br>\n|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|---good <br>\n|__test <br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|--- good&fault case <br>","metadata":{}},{"cell_type":"code","source":"class MVTec_AD():\n    def __init__(self, DB_PATH, OUT_PATH, size=(86,86), flatten=True):\n        '''\n        DB_PATH : str. e.g. {workspace}/dataset\n        size : (int, int). default (86, 86)\n        flatten : bool. [num_of_img_per_class, h*w]/[num_of_img_per_class, h, w]\n        '''\n        self.DB_PATH = DB_PATH\n        self.out_path = OUT_PATH\n        self.size, self.flatten = size, flatten\n\n        self.class_names = ['bottle', 'cable', 'capsule', 'carpet', 'grid',\n                            'hazelnut', 'leather', 'metal_nut', 'pill', 'screw',\n                            'tile', 'transistor', 'wood', 'zipper']\n\n\n    def read_mvtec(self, cls=\"bottle\", mode='train'):\n        '''\n        input\n          cls : str. class name.\n          mode : str. {train, test, val}\n        output\n          db_dict : dict. \n                  if val in mode\n                   {'imgs':[num_of_img_per_class, h*w or h, w], 'labels':gt, 'class_name':class_name}\n                  else\n                   {'imgs':[num_of_img_per_class, h*w or h, w], 'class_name':class_name}\n        '''\n\n        # Load cache data.\n        CACHE_PATH = self.out_path + '/cache'\n        cache_ = join(CACHE_PATH, mode)\n        if self.flatten: cache_+= 't_'\n        cache_+=f'{str(self.size)}_{cls}.pkl'\n\n        # if exist cache data return data\n        if os.path.isfile(cache_):\n            with open(cache_, 'rb') as f:\n                data = pickle.load(f)\n            return data\n        # else load imgs and cache data\n        else:\n            if os.path.isdir(CACHE_PATH)!=True:\n                os.mkdir(CACHE_PATH)\n\n            # load imgs\n            data = edict()  \n            if mode == 'val':\n                csv = pd.read_csv(join(self.DB_PATH, cls, 'val.csv'))\n                data.imgs = [f'{self.DB_PATH}/{cls}/test/{csv.iloc[id, 0]:03d}.png' for id in csv.index]\n                data.labels = [csv.iloc[id, 1] for id in csv.index]\n            else:\n                data.imgs = sorted(glob(join(self.DB_PATH, cls, mode, '*.png')))\n\n            data.class_name = cls\n\n            data = self.read_img(data)\n\n            # cache data\n            with open(cache_, \"wb\") as f:\n                pickle.dump(data, f)\n\n            return data\n        \n    def read_img(self, db_dict):\n        im_path = db_dict.imgs\n        len_imgs = len(im_path)\n\n        db_dict.imgs = np.array([np.array(Image.open(im).convert(\"L\").resize(self.size)) for im in im_path])\n        if self.flatten:\n            db_dict.imgs = db_dict.imgs.reshape(len_imgs,-1)\n      \n        return db_dict","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:34:57.775201Z","iopub.execute_input":"2021-06-07T13:34:57.775523Z","iopub.status.idle":"2021-06-07T13:34:57.791512Z","shell.execute_reply.started":"2021-06-07T13:34:57.775485Z","shell.execute_reply":"2021-06-07T13:34:57.790398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\ndef title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:34:57.793846Z","iopub.execute_input":"2021-06-07T13:34:57.794277Z","iopub.status.idle":"2021-06-07T13:34:57.81084Z","shell.execute_reply.started":"2021-06-07T13:34:57.794229Z","shell.execute_reply":"2021-06-07T13:34:57.80979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load val, train data\nDB_PATH = \"../input/2021-ml-tp04/MVTecAD\"\nOUT_PATH = \"./\"\n\nsize = (86, 86)\ncls = 'bottle'\ndataset = MVTec_AD(DB_PATH, OUT_PATH, flatten=True, size=size)\n\nval = dataset.read_mvtec(cls=cls, mode='val')\ntrain = dataset.read_mvtec(cls=cls, mode='train')\ntest = dataset.read_mvtec(cls=cls, mode='test')\n\n# 해당 데이터 셋은 비지도 학습을 위한 데이터 셋이기 때문에,\n# train data는 정상 케이스의 영상들로만 구성되어 있습니다.\n# 베이스 코드에서는 train의 key 정보와 같이 train의 라벨을 주어주지 않습니다.\n# 데이터 구성에 대한 내용은 https://colab.research.google.com/drive/1pdgvoPs3KDLq6pV9oLxkXDh6waEp76HT?usp=sharing에 있습니다. 해당 부분도 보시기 바랍니다.\n# 위의 URL 주소는 Dataset 설명란의 MVTecAD_colab과 동일한 주소입니다.\n# ++ 만약에 train의 라벨이 필요하신 분들은 각 클래스의 train 폴더의 data.csv를 사용하시기 바랍니다.\nprint(train.keys())\nprint(val.keys())\nprint(test.keys())","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:34:57.812131Z","iopub.execute_input":"2021-06-07T13:34:57.812457Z","iopub.status.idle":"2021-06-07T13:35:14.92844Z","shell.execute_reply.started":"2021-06-07T13:34:57.812425Z","shell.execute_reply":"2021-06-07T13:35:14.927379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Reconstruction based anomaly detection","metadata":{}},{"cell_type":"code","source":"# [1] Reconstruction based anomaly detection\n\nn_components = 0.8\n    \n\nscaler = StandardScaler() # StandardScaler() 선언\ntrain_sc = scaler.fit_transform(train.imgs) # StandardScaler()를 train.imgs에 학습 및 적용\nval_sc = scaler.transform(val.imgs) # StandardScaler()를 val.imgs에 적용\n\npca = PCA(n_components = n_components, random_state=777) # PCA 선언과 파라미터 설정\npca.fit(train_sc) # PCA에 정규화한 train 데이터를 학습\n\nval_pca = pca.transform(val_sc) # 정규화된 val.imgs PCA transform\n\nnum_data_imgs = 53 # size of original img\nimg_width = size[0] # img width\nimg_height = size[1] # img height\n\nReconstruction_imgs = pca.inverse_transform(val_pca)\n# 정규화, PCA를 적용했던 val_pca를 PCA의 inverse_trasform으로 복원하고 Reconstruction_imgs 에 저장\nReconstruction_imgs = scaler.inverse_transform(Reconstruction_imgs).reshape(num_data_imgs, img_width, img_height)\n#  Reconstruction_imgs를 StrandardScaler()의 inverse_trasform으로 복원하고\n# 오리지널 imgs의 크기로 reshape 한 후 Reconstruction_imgs 에 다시 저장\n\n# Reconstruction_error = Original imgs - Reconstruction_imgs\n\nori = val.imgs.reshape(num_data_imgs, img_width, img_height)\n# reshape val.imgs\nOriginal_imgs = ori\nReconstruction_error = Original_imgs - Reconstruction_imgs\n\n# score & predict normal/abnormal\nmin_max_scaler = MinMaxScaler()\n\ncls_score = Reconstruction_error.sum(axis=1).sum(axis=1)\ncls_score = min_max_scaler.fit_transform(cls_score.reshape(-1, 1))\n\ny_pred = cls_score\n\nth = 0.5\ny_pred[cls_score < th] = -1\ny_pred[cls_score > th] = 1\ny_pred = y_pred.reshape(-1)\n\ngt_list = np.asarray(val.labels)\nfpr, tpr, _ = roc_curve(gt_list, cls_score)\nimg_roc_auc = roc_auc_score(gt_list, cls_score)\n\nplt.plot(fpr, tpr, label='%s ROCAUC: %.3f' % (cls, img_roc_auc))\nplt.title(f'{cls} ROCAUC: {img_roc_auc:.3f}')\n\nprint(f'{cls} ROCAUC: {img_roc_auc:.3f}')\n\n\ntarget_names = {-1:'abnormal', 1:'normal'}\norginal_title = [f'{cls}_{target_names[label]}' for label in gt_list]\nplot_gallery(ori, orginal_title, size[0], size[1])\n\n\nprediction_titles = [title(y_pred, gt_list, target_names, i)\n                     for i in range(y_pred.shape[0])]\n# Reconstruction img                     \nplot_gallery(Reconstruction_imgs, prediction_titles, size[0], size[1])\n# Reconstruction error img                     \nplot_gallery(Reconstruction_error, prediction_titles, size[0], size[1])\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:35:14.929827Z","iopub.execute_input":"2021-06-07T13:35:14.93011Z","iopub.status.idle":"2021-06-07T13:35:17.629761Z","shell.execute_reply.started":"2021-06-07T13:35:14.93008Z","shell.execute_reply":"2021-06-07T13:35:17.629037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Embedding feature based anomaly detection","metadata":{}},{"cell_type":"code","source":"DB_PATH = \"../input/2021-ml-tp04/MVTecAD\"\n\nOUT_PATH = './'\nFIG_PATH = join(OUT_PATH, \"fig\")\nif os.path.isdir(FIG_PATH)!=True:\n    os.mkdir(FIG_PATH)\n\nfrom sklearn.svm import OneClassSVM\n\ndataset = MVTec_AD(DB_PATH, OUT_PATH, flatten=True, size=size)\n\nn_components=180\nsize = (86, 86)\n\ny_preds = []\ny_test = []\n\nroc_auc = dict()\ntotal_roc_auc = []\n\nplt.figure(figsize=[10, 10])\n# print(f'\\n=====clf:{clf_name}, DA:{dr_name}=====')\n# total_roc_auc = []\n\npbar = tqdm(dataset.class_names)\n\nsample_submit = pd.read_csv('../input/2021-ml-tp04/sample_submit.csv', index_col=0)\n\ni=0\n\nfor cls in pbar:\n    train = dataset.read_mvtec(cls=cls, mode='train')\n    val = dataset.read_mvtec(cls=cls, mode='val')\n    test = dataset.read_mvtec(cls=cls, mode='test')\n\n    \n    clf = OneClassSVM(kernel=\"rbf\", gamma = 0.001, nu = 0.005) # 모델 파라미터 조정\n    \n    # Parameter histories\n    # 아래는 성능을 올리기 위해 사용했던 파라미터 조정값들 입니다.\n    #kernel=\"rbf\", gamma = 0.001, nu = 0.01 // kaggle: 0.53167\n    #kernel=\"rbf\", gamma = 0.005, nu = 0.01 // kaggle: 0.55846\n    #kernel=\"rbf\", gamma = 0.01, nu = 0.01 // kaggle: 0.57840\n    #kernel=\"rbf\", gamma = 0.05, nu = 0.01 // kaggle: 0.53205\n    #kernel=\"rbf\", gamma = 0.02, nu = 0.01 // kaggle: 0.50525\n    #kernel=\"rbf\", gamma = 1, nu = 0.01 // kaggle: 0.51593\n    #kernel=\"rbf\", gamma = 0.01, nu = 0.05 // kaggle: 0.55829\n    #kernel=\"rbf\", gamma = 0.01, nu = 0.005 // kaggle: 0.54271\n    #kernel=\"rbf\", gamma= 'auto', nu=0.01 // kaggle: 0.51190\n    #kernel=\"linear\", gamma= '0.001', nu=0.01 // kaggle: 0.50098\n    \n    # \n    # kernel=\"rbf\", gamma = 0.001, nu = 0.01 // kaggle: 0.71074\n    # kernel=\"rbf\", gamma = 0.01, nu = 0.01 // kaggle: 0.70439\n    # kernel=\"rbf\", gamma = 0.001, nu = 0.005 // kaggle: 0.71098\n    \n    \n    #pca = PCA(n_components = n_components, random_state = 777)\n    pca = PCA(n_components = n_components, random_state = 777, svd_solver = 'randomized', whiten = True)\n    \n    # Parameter histories\n    # n_components = n_components, random_state = 777, svd_solver = 'randomized', whiten = True // kaggle: 0.57840\n    # pca = PCA(n_components = n_components, random_state = 777)\n    # n_components = n_components, random_state = 777 // kaggle: 0.57840 \n    \n    #sc = MinMaxScaler()\n    sc = StandardScaler() # StandardScaler 선언\n    train_sc = sc.fit_transform(train.imgs) # train.imgs 학습 및 변환\n    val_sc = sc.transform(val.imgs) # val.imgs 변환\n\n    \n    train_pca = pca.fit_transform(train_sc) # pca에 정규화된 train_sc를 학습 및 변환\n    val_pca = pca.transform(val_sc) # val_sc를 pca 변환\n\n    \n    clf.fit(train_pca) # 모델에 정규화와 차원축소를 거친 train_pca를 학습시킵니다.\n    y_pred = clf.predict(val_pca) # val_pca를 예측하여  y_pred에 저장합니다.\n    cls_score_val = clf.score_samples(val_pca) # 점수를 cls_score_val에 저장합니다.\n\n    test_sc = sc.transform(test.imgs) # test.imgs를 정규화\n    test_pca = pca.transform(test_sc) # test.imgs를 차원축소\n    cls_score = clf.score_samples(test_pca) # 점수를 cls_score에 저장\n\n    \n    # eval~valid data\n    y_preds.append(y_pred)\n    gt_list = np.array(val.labels)\n    fpr, tpr, _ = roc_curve(gt_list, cls_score_val)\n    img_roc_auc = roc_auc_score(gt_list, cls_score_val)\n    total_roc_auc.append(img_roc_auc)\n    plt.plot(fpr, tpr, label='%s ROCAUC: %.3f' % (cls, img_roc_auc))\n    \n    sample_submit['score'][i:i+cls_score.shape[0]] = scaler.fit_transform(cls_score.reshape(-1,1)).reshape(-1)\n    i = i+cls_score.shape[0]\n\nprint('Average ROCAUC: %.3f' % np.mean(total_roc_auc))\nclf_name = type(clf).__name__\npca_name = type(pca).__name__\nplt.title(f'{clf_name}_{pca_name}_{n_components}\\nAverage image ROCAUC: {np.mean(total_roc_auc):.3f}' )\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc='lower right')\n\n# Result Figure\nplt.savefig(f'{FIG_PATH}/{clf_name}_{pca_name}_{n_components}.jpg')\n\n# Submit CSV\nsample_submit.to_csv(f'./{clf_name}_{pca_name}_{n_components}.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:35:17.63086Z","iopub.execute_input":"2021-06-07T13:35:17.631283Z","iopub.status.idle":"2021-06-07T13:42:13.727276Z","shell.execute_reply.started":"2021-06-07T13:35:17.631251Z","shell.execute_reply":"2021-06-07T13:42:13.725835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = dataset.read_mvtec(cls=cls, mode='test')\nnp.sqrt(test.imgs.shape[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:42:13.731204Z","iopub.execute_input":"2021-06-07T13:42:13.731701Z","iopub.status.idle":"2021-06-07T13:42:13.74023Z","shell.execute_reply.started":"2021-06-07T13:42:13.731647Z","shell.execute_reply":"2021-06-07T13:42:13.739216Z"},"trusted":true},"execution_count":null,"outputs":[]}]}