# Voice-based Emotion Classification Project => [Full Code](https://github.com/piso7/2021.MachineLearning.TermProjects/blob/main/DepressionPredictionProject/kkw-ml-teamproject_1.ipynb)

## 1D Voice data description (Feature Extraction) prior knowledge
* Since audio data is generated by the vibration of air molecules, it can be expressed as a graph with period and amplitude as shown in the figure below. In speech data, the x-axis means time and the y-axis means amplitude, and there is a period, which is the time from one point to the next. This period can be expressed as frequency, and the unit of Hz is used to indicate how many cycles occur in 1 second. Mainly, frequency represents the treble and bass of a sound, and Amplitude represents the loudness of the sound. As you can see from the graph, voice data is continuous data, so it must be changed to a discrete value to be used for model training.
<img src="https://i.imgur.com/ln6hAUX.png"  width="50%" height="50%"/>

* In order to convert voice data to discrete type, sampling and quantization are largely performed. When sampling, the number of samples to extract and use per second is called the sample rate. The higher the sample rate, the more similar the original sound can be, but it takes up more memory space. In the case of quantization, it is used to express continuous information as discrete information, and as the bit size of quantization increases, information similar to the original is obtained.

* Even though the voice information has been subjected to sampling and quantization, it is difficult to use it as an input for a model. Since various frequencies are mixed in the voice information and it has a lot of information, it is essential to select a feature that represents the representative nature of the voice from among them rather than using the voice data directly.

* In other words, depending on how features are extracted from the data, the process of designing the features should be carried out because it can have a big impact on the performance of the model.

* In this project, Mel-Spectrogram and MFCC (Mel_Frequency Ceptral Coefficients), which are the most representative voice features, are used for voice classification.

<img src="https://i.imgur.com/Ch4MFTx.png"  width="50%" height="50%"/>

## Step 1. Data Preprocessing
<img src="https://user-images.githubusercontent.com/62230550/165679392-0161d263-3c51-4057-9a87-90d085329276.png"  width="50%" height="50%"/>
Extract 'Sleep', 'Social', and 'Activity' data from the survey response json file and parse the actual values into index values.

## Step 2. Feature Extraction
<img src="https://user-images.githubusercontent.com/62230550/165681127-ee398d0c-6e7c-4c29-bd62-b71e5609ea0c.png"  width="50%" height="50%"/>
Extract statistical features from the previously parsed EMA response data.   

* Data is divided among trian/test users based on the previously added user id (uid).  
* Then, statistical features are extracted from the user's data using the describe function provided by pandas.  

Fill in the missing values of the data with the average value using the imputer.  
<img src="https://user-images.githubusercontent.com/62230550/165681895-27653da8-54e7-4e69-8ce9-881034df2337.png"  width="70%" height="70%"/>

## Step 3. Model training and prediction
![image](https://user-images.githubusercontent.com/62230550/165683463-bb0416e0-6209-4391-84d3-8600c09c1fc7.png)

* Depression prediction using only the sleep feature  
* Depression prediction using only the activity features
* Depression prediction using only the social features
* Depression prediction using all features

## Conclusion
* It can be confirmed that depression classification with better performance is possible by fusion of features extracted from various EMA data.
* Through this, the performance difference according to the actual feature representation can be confirmed.
