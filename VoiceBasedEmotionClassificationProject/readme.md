# Voice-based Emotion Classification Project => [Full Code](https://github.com/piso7/2021.MachineLearning.TermProjects/blob/main/VoiceBasedEmotionClassificationProject/kkw-ml-teamproject-3.ipynb)
The project was conducted on the KAGGLE platform.  

## 1D Voice data description (Feature Extraction) prior knowledge
* Since audio data is generated by the vibration of air molecules, it can be expressed as a graph with period and amplitude as shown in the figure below. In speech data, the x-axis means time and the y-axis means amplitude, and there is a period, which is the time from one point to the next. This period can be expressed as frequency, and the unit of Hz is used to indicate how many cycles occur in 1 second. Mainly, frequency represents the treble and bass of a sound, and Amplitude represents the loudness of the sound. As you can see from the graph, voice data is continuous data, so it must be changed to a discrete value to be used for model training.
<img src="https://i.imgur.com/ln6hAUX.png"  width="50%" height="50%"/>

* In order to convert voice data to discrete type, sampling and quantization are largely performed. When sampling, the number of samples to extract and use per second is called the sample rate. The higher the sample rate, the more similar the original sound can be, but it takes up more memory space. In the case of quantization, it is used to express continuous information as discrete information, and as the bit size of quantization increases, information similar to the original is obtained.

* Even though the voice information has been subjected to sampling and quantization, it is difficult to use it as an input for a model. Since various frequencies are mixed in the voice information and it has a lot of information, it is essential to select a feature that represents the representative nature of the voice from among them rather than using the voice data directly.

* In other words, depending on how features are extracted from the data, the process of designing the features should be carried out because it can have a big impact on the performance of the model.

* In this project, Mel-Spectrogram and MFCC (Mel_Frequency Ceptral Coefficients), which are the most representative voice features, are used for voice classification.

<img src="https://i.imgur.com/Ch4MFTx.png"  width="50%" height="50%"/>

## Feature Extraction

### 1. Obtaining a Spectrogram
#### Step 1. Windowing
* To give time invariant properties, the signals are cut at regular intervals. This is because, if the features are calculated without considering the temporal characteristics, meaningful characteristics related to time cannot be found.
* According to the research result that the human voice cannot change the pronunciation of the current speaking within 20-40 ms, the regular interval is set to 20-40 ms.
#### Step 2. Fast Fourier Transform
* Fast Fourier Transform (FFT) is performed on time-segmented speech data.
* As mentioned earlier, a voice signal is composed of time (x-axis) and amplitude (y-axis). In this time domain, numerous frequencies exist, making it difficult to analyze signal information and design it as a feature.
* Therefore, the signal is transformed from the time domain to the frequency domain using the Fourier transform.
* The information converted into the frequency domain is called 'Spectrum'. The spectrum has a unique harmonic structure, and by inferring the harmonic structure, you can design the unique characteristics of the sound.

#### Step 3. Short Time Fourier Transform
* Although FFT can be applied to the entire signal data, in this case, temporal information cannot be used due to the nature of the time-dependent signal.
* However, since the audio signal is divided into frames through the windowing process, FFT is applied to each frame and then connected in time order again to obtain a spectrum considering the temporal characteristics. This is called a Short Time Fourier Transform (STFT).
* In other words, it is possible to create data that includes not only frequency and magnitude information of spectrum, but also time frame information through STFT. We define this as a spectrogram

### 2. Obtaining a Mel-Spectrogram
* Mel-Filter spectrogram is generated by applying Mel Filter to the spectrogram obtained by STFT(Short Time Fourier Transform).
* Humans are more sensitive to low frequencies than to high frequencies. For example, humans can distinguish the difference between 2000Hz and 4000Hz sound well, but cannot distinguish between 13000H and 15000Hz sounds.
* It is the Mel Scale that is expressed by modeling such a human hearing organ.
* It converts the spectrogram to mel-scale by applying mel-filter to the spectrogram obtained through STFT.

### 3. Finding MFCC using DCT(Discrete cosine transform)
* By performing discrete cosine transform (DCT) operation on Mel-Spectrogram, MFCC features can be finally extracted.

## Model training and prediction
![image](https://user-images.githubusercontent.com/62230550/165698092-719dd539-586b-4f0b-a8d0-e897b2e41fbf.png)

Use the Random Forest Classifier of sklearn to learn features and derive results.

![image](https://user-images.githubusercontent.com/62230550/165698735-979c70b5-e98d-4b7d-a25c-6617adc6fe28.png)


