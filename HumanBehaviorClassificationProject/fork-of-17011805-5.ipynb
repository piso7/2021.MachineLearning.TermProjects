{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2021-05-29T08:02:08.045706Z","iopub.execute_input":"2021-05-29T08:02:08.046425Z","iopub.status.idle":"2021-05-29T08:02:09.606223Z","shell.execute_reply.started":"2021-05-29T08:02:08.04632Z","shell.execute_reply":"2021-05-29T08:02:09.605318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tqdm\nimport time\nimport cv2\nimport pickle\nimport torch\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.svm import SVC\nfrom scipy.stats import mode\nfrom sklearn.decomposition import PCA\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:02:09.626925Z","iopub.execute_input":"2021-05-29T08:02:09.62725Z","iopub.status.idle":"2021-05-29T08:02:12.143571Z","shell.execute_reply.started":"2021-05-29T08:02:09.627218Z","shell.execute_reply":"2021-05-29T08:02:12.142597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 베이스라인 달성을 위한 파라미터 제공\narg_img_size = (128, 128)\narg_dense_sift = True\nargs_local_cluster = 200\nargs_global_cluster = 200\nnum_frame = 5\nargs_aggr = \"vlad\" # \"vlad\" or \"bow\"\npca_vlad = 128","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:02:12.144724Z","iopub.execute_input":"2021-05-29T08:02:12.145023Z","iopub.status.idle":"2021-05-29T08:02:12.149648Z","shell.execute_reply.started":"2021-05-29T08:02:12.144996Z","shell.execute_reply":"2021-05-29T08:02:12.148739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Video preprocessing and frame-specific feature point extraction","metadata":{}},{"cell_type":"code","source":"# train 비디오의 행동 분류 label read\nroot = \"/kaggle/input/2021-ml-tp4/\"\ntrain_csv = os.path.join(root, \"train_label.csv\")\ntrain_csv = pd.read_csv(train_csv)\ntrain_csv_arr = np.asarray(train_csv)\n\n# 데이터 셋에 존재하는 행동 분류 정보 read\nclassinfo = os.path.join(root, \"class_info.csv\")\nclassinfo = pd.read_csv(classinfo)\nclassinfo_arr = np.asarray(classinfo)\n\n\ntrain_path = os.path.join(root, \"train\")\ntest_path = os.path.join(root, \"test\")\n\n# train 비디오 경로\ntrain_list = os.listdir(train_path)\ntrain_list.sort()\ntrain_list = [os.path.join(train_path, i) for i in train_list]\n\n# test 비디오 경로\ntest_list = os.listdir(test_path)\ntest_list.sort()\ntest_list = [os.path.join(test_path, i) for i in test_list]","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:02:12.150776Z","iopub.execute_input":"2021-05-29T08:02:12.151272Z","iopub.status.idle":"2021-05-29T08:02:12.190197Z","shell.execute_reply.started":"2021-05-29T08:02:12.151235Z","shell.execute_reply":"2021-05-29T08:02:12.189426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def video_to_frame(video_path, size, num_frame):\n    \n    #########################################################\n    ## The function that extract frames from the video\n    ## \n    ## Input \n    ##     video_path : 한 비디오의 경로\n    ##     size : 비디오 내의 프레임을 읽을 때, 원하는 해상도 크기\n    ##     num_frames : 한 비디오 내에서 읽을 프레임의 수\n    ##\n    ## Output\n    ##     frames : 읽고 저장한 총 프레임\n    #########################################################\n    \n    cap = cv2.VideoCapture(video_path)\n    \n    total_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    sel_ind = np.linspace(0, total_frame-1, num_frame).astype(\"int\")\n    \n    \n    num=0\n    frames = []\n    for i in range(total_frame):\n        \n        # 읽을 프레임 인덱스의 경우 프레임 읽어 메모리에 저장, 아닐 경우 지나감\n        if i in sel_ind:\n            res, frame = cap.read()\n            # 원하는 해상도로 조절 및 grayscale로 변환\n            frame = cv2.resize(frame, size, interpolation = cv2.INTER_CUBIC)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            frames.append(frame)\n        else:\n            res = cap.grab()        \n    cap.release()\n    frames = np.asarray(frames)\n\n    return frames","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:02:12.192726Z","iopub.execute_input":"2021-05-29T08:02:12.193184Z","iopub.status.idle":"2021-05-29T08:02:12.200686Z","shell.execute_reply.started":"2021-05-29T08:02:12.193143Z","shell.execute_reply":"2021-05-29T08:02:12.199855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def computeSIFT(data, dense=False):\n    \n    #########################################################\n    ## The function that extracts visual words from frames within a video as SIFT or DenseSIFT\n    ## \n    ## Input \n    ##     data : 한 비디오에서 읽고 저장한 프레임\n    ##     dense : SIFT or DenseSIFT 사용 여부\n    ##\n    ## Output\n    ##     x : 프레임에 대해 추출된 특징점(visual word), dict 형태 -> x[0]이면 0번째 인덱스 프레임의 특징점(visual word) [n,128] 확인 가능\n    ##     x : Visual word extracted for the frame, dict form -> x[0] allows you to check the visual word [n,128] of the 0th index frame        \n    #########################################################\n    \n    x = {}\n    for i in range(0, len(data)):\n        if dense:\n            img = data[i]\n            step_size = 8\n            kp = [cv2.KeyPoint(x, y, step_size) for x in range(0, img.shape[0], step_size) for y in range(0, img.shape[1], step_size)]\n            \n\n            # 기본 SIFT 와 동일하게 정의 \n            # 기본 SIFT 에서는 detectAndCompute를 사용했지만 \n            # Dense SIFT는 위에서 생성한 keypoint를 사용해 compute 만을 진행 \n            \n            sift = cv2.SIFT_create()\n            # SIFT 추출기를 생성합니다.\n            kp, desc = sift.compute(img, kp)\n            # img 와 keypoint를 이용하여 compute를 진행하고 keypoint와 기술자를 반환받습니다.\n            \n        else:\n            sift = cv2.SIFT_create()\n            img = data[i]\n            kp, desc = sift.detectAndCompute(img, None)\n        x.update({i : desc})\n\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:02:12.20322Z","iopub.execute_input":"2021-05-29T08:02:12.203529Z","iopub.status.idle":"2021-05-29T08:02:12.215196Z","shell.execute_reply.started":"2021-05-29T08:02:12.203501Z","shell.execute_reply":"2021-05-29T08:02:12.214131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train 비디오에서 프레임 추출 및 특징점(visual word) 추출, dict 형태로 train_local_desc[비디오 경로]이면 해당하는 비디오에서 추출한 모든 특징점(visual word) 확인 가능\n# Extract frame and extract visual word from train video, test_local_desc[video path] in dict form, and see all visual words from that video\n\ntrain_local_desc = {}\nfor vi, vid_path in enumerate(tqdm.tqdm(train_list, desc=\"Extract {} in train data\".format(\"dsift\" if arg_dense_sift else \"sift\"))):\n    curr_frame = video_to_frame(vid_path, arg_img_size, num_frame)\n    local_desc = computeSIFT(curr_frame, arg_dense_sift)\n    train_local_desc.update({vid_path : local_desc})\n\n# test 비디오에서 프레임 추출 및 특징점(visual word) 추출, dict 형태로 test_local_desc[비디오 경로]이면 해당하는 비디오에서 추출한 모든 특징점(visual word) 확인 가능\n# Extract frame and extract visual word from test video, test_local_desc[video path] in dict form, and see all visual words from that video\n\ntest_local_desc = {}\nfor vi, vid_path in enumerate(tqdm.tqdm(test_list, desc=\"Extract {} in test data\".format(\"dsift\" if arg_dense_sift else \"sift\"))):\n    curr_frame = video_to_frame(vid_path, arg_img_size, num_frame)\n    local_desc = computeSIFT(curr_frame, arg_dense_sift)\n    test_local_desc.update({vid_path : local_desc})\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:02:12.216899Z","iopub.execute_input":"2021-05-29T08:02:12.217403Z","iopub.status.idle":"2021-05-29T08:04:51.682802Z","shell.execute_reply.started":"2021-05-29T08:02:12.217345Z","shell.execute_reply":"2021-05-29T08:04:51.68172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nAggregate SIFT descriptor\")\nstart = time.time()\n\n\n# train 비디오 별로 나눠진 특징점(visual word)들을 [n,128]형태로 모음, 모아진 특징점(visual word)들의 정보(비디오 내의 몇번째 프레임에서 나온 특징점인지)는 \n# 같은 인덱스의 train_frame_total에서 확인 가능 및 비디오 내의 특정 프레임에서 나온 특징점(visual word)의 수는 train_local_info에서 확인 가능\n\n# Train The visual words divided by video are collected in the form of [n,128], and the information of the visual words (which frame in the video is the feature point)\n# train_frame_total with the same index and the number of visual words from a particular frame in the video are train_local_info\n\ntrain_frame_total = []\ntrain_local_desc_total = []\ntrain_local_info = {}\nfor k, v in train_local_desc.items():\n    for kk, vv in v.items():\n        l_num = 0\n        if vv is not None:\n            train_local_desc_total.extend(vv)\n            train_frame_total.extend([k+\", \"+str(kk)] * len(vv))\n            l_num = len(vv)\n        train_local_info.update({k+\", \"+str(kk) : l_num})\ntrain_local_desc_total = np.asarray(train_local_desc_total)\ntrain_frame_total = np.asarray(train_frame_total)\n\n\n# test 비디오 별로 나눠진 특징점(visual word)들을 [n,128]형태로 모음, 모아진 특징점(visual word)들의 정보(비디오 내의 몇번째 프레임에서 나온 특징점인지)는 \n# 같은 인덱스의 test_frame_total에서 확인 가능 및 비디오 내의 특정 프레임에서 나온 특징점(visual word)의 수는 test_local_info에서 확인 가능\ntest_frame_total = []\ntest_local_desc_total = []\ntest_local_info = {}\nfor k, v in test_local_desc.items():\n    for kk, vv in v.items():\n        l_num = 0\n        if vv is not None:\n            test_local_desc_total.extend(vv)\n            test_frame_total.extend([k+\", \"+str(kk)] * len(vv))\n            l_num = len(vv)\n        test_local_info.update({k+\", \"+str(kk) : l_num})\ntest_local_desc_total = np.asarray(test_local_desc_total)\ntest_frame_total = np.asarray(test_frame_total)\n\n\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:04:51.684013Z","iopub.execute_input":"2021-05-29T08:04:51.684282Z","iopub.status.idle":"2021-05-29T08:04:56.350059Z","shell.execute_reply.started":"2021-05-29T08:04:51.684255Z","shell.execute_reply":"2021-05-29T08:04:56.349361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Frame Feature for Video Feature","metadata":{}},{"cell_type":"code","source":"def clustering(train_desc, test_desc=None, n_clusters = 200):\n    #########################################################\n    ## 모든 특징점들 중, 대표 특징점(codebook)을 선정하는 함수\n    ## Of all feature points, a function that selects a representative feature point (codebook)\n    ##\n    ## Input \n    ##     train_desc : 모든 train 비디오의 모든 프레임에서 추출한 특징점(visual word)들\n    ##     test_desc : 모든 test 비디오의 모든 프레임에서 추출한 특징점(visual word)들\n    ##     n_clusters : 대표 특징점(codebook)의 수\n    ##\n    ## Output\n    ##     train_pred : 대표 특징점(codebook)에 대해 train_desc가 할당된 위치\n    ##     test_pred : 대표 특징점(codebook)에 대해 train_desc가 할당된 위치\n    ##     clusters : 대표 특징점(codebook)\n    ##     kmeans : kmeans 인스턴스\n    #########################################################\n    \n    \n    \n    # Selection of representative feature points (codebook)\n    \n    kmeans = MiniBatchKMeans(n_clusters = n_clusters, random_state = 0)\n    # MiniBatchKMeans 를 초기화하고 파라미터 설정, n_clusters 만큼의 대표 특징점을 선정합니다.\n    kmeans.fit(train_desc)\n    # train_desc 즉 train video에서 추출한 특징점을 모델에 학습시킵니다.\n    clusters = kmeans.cluster_centers_\n    # 대표 특징점들을 clusters에 저장합니다.\n\n    \n    \n    train_pred = kmeans.predict(train_desc)\n    if test_desc is not None:\n        test_pred = kmeans.predict(test_desc)\n    else:\n        test_pred = None\n    return train_pred, test_pred, clusters, kmeans","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:04:56.351035Z","iopub.execute_input":"2021-05-29T08:04:56.351447Z","iopub.status.idle":"2021-05-29T08:04:56.357288Z","shell.execute_reply.started":"2021-05-29T08:04:56.351414Z","shell.execute_reply":"2021-05-29T08:04:56.356326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a representative feature point (codebook) with visual words extracted from all frames of all train videos.\n# Assign a visual word extracted from all frames of the train video and a visual word extracted\n# from all frames of the test video to a representative feature point (codebook).\ntrain_local_alloc, test_local_alloc, local_codebook, local_kmeans = clustering(train_local_desc_total, test_local_desc_total, args_local_cluster)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-29T08:04:56.358432Z","iopub.execute_input":"2021-05-29T08:04:56.358696Z","iopub.status.idle":"2021-05-29T08:08:06.272204Z","shell.execute_reply.started":"2021-05-29T08:04:56.358671Z","shell.execute_reply":"2021-05-29T08:08:06.270964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def VLAD(X, alloc, centers):\n    #########################################################\n    ## Function to describe VLAD feature which is image feature\n    ## \n    ## Input \n    ##     X : Visual words in a frame\n    ##     alloc : Location where the visual words of a frame are assigned to the representative feature points (codebook)\n    ##     centers : codebook\n    ##\n    ## Output\n    ##     V : VLAD feature\n    #########################################################\n    \n    m,d = X.shape\n    k = centers.shape[0]\n    \n    # VLAD feature를 담기 위한 변수\n    V = np.zeros([k,d])\n\n    for i in range(k):\n        if np.sum(alloc == i)>0:\n            \n            # Using the visual word X extracted from the image and the information alloc assigned to them as representative feature points (codebook),\n            # Calculate the vector sum of the visual words assigned to the same representative feature point (codbook) and store them in V[i]\n            # VLAD(test_local_desc_total[vi:vi+v], test_local_alloc[vi:vi+v], local_codebook)\n            \n            V[i] = np.sum(X[alloc == i, :] - centers[i], axis = 0)\n\n            ######################################################################\n    \n    V = V.flatten()\n    V = np.sign(V)*np.sqrt(np.abs(V))\n    if np.sqrt(np.dot(V,V))!=0:\n        V = V/np.sqrt(np.dot(V,V))\n    return V\n\n\ndef BoW(alloc, n_cluster):\n    #########################################################\n    ## Function to describe BoW feature which is image feature\n    ## \n    ## Input \n    ##     alloc : Location where the visual words of a frame are assigned to the representative feature points (codebook)\n    ##     n_cluster : Number of representative feature points (codebooks)\n    ##\n    ## Output\n    ##     V : BoW feature\n    #########################################################\n    \n    # Calculate the histogram of the information alloc that the visual word extracted from the image is assigned as the representative feature point (codebook)\n    # np.histogram\n    V, _ = np.histogram(alloc, bins = range(n_cluster + 1))\n\n    return V","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:08:06.273823Z","iopub.execute_input":"2021-05-29T08:08:06.274248Z","iopub.status.idle":"2021-05-29T08:08:06.283921Z","shell.execute_reply.started":"2021-05-29T08:08:06.274197Z","shell.execute_reply":"2021-05-29T08:08:06.282944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nAllocate center & Descript local histogram\")\nstart = time.time()\n\n# Train 비디오 내의 프레임 별로 이미지 feature 기술 -> train_global_desc\n# 각 이미지 feature의 정보(속한 비디오 이름, 비디오 내의 인덱스) -> train_global_desc_key\ntrain_global_desc = []\ntrain_global_desc_key = []\nvi=0\nfor k, v in train_local_info.items():\n    if v!=0:\n        if args_aggr==\"bow\":            \n            hist_desc = BoW(train_local_alloc[vi:vi+v], args_local_cluster)\n        elif args_aggr==\"vlad\":\n            hist_desc = VLAD(train_local_desc_total[vi:vi+v], train_local_alloc[vi:vi+v], local_codebook)\n        else:\n            import pdb; pdb.set_trace()\n\n        vi+=v\n        train_global_desc.append(hist_desc)\n        train_global_desc_key.append(k)\ntrain_global_desc = np.asarray(train_global_desc)\ntrain_global_desc_key = np.asarray(train_global_desc_key)\n\n\n# Test 비디오 내의 프레임 별로 이미지 feature 기술 -> test_global_desc\n# 각 이미지 feature의 정보(속한 비디오 이름, 비디오 내의 인덱스) -> test_global_desc_key\ntest_global_desc = []\ntest_global_desc_key = []\nvi=0\nfor k, v in test_local_info.items():\n    if v!=0:\n        if args_aggr==\"bow\":\n            hist_desc = BoW(test_local_alloc[vi:vi+v], args_local_cluster)\n        elif args_aggr==\"vlad\":\n            hist_desc = VLAD(test_local_desc_total[vi:vi+v], test_local_alloc[vi:vi+v], local_codebook)\n        else:\n            import pdb; pdb.set_trace()\n\n        vi+=v\n        test_global_desc.append(hist_desc)\n        test_global_desc_key.append(k)\ntest_global_desc = np.asarray(test_global_desc)\ntest_global_desc_key = np.asarray(test_global_desc_key)\n\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:08:06.285398Z","iopub.execute_input":"2021-05-29T08:08:06.285966Z","iopub.status.idle":"2021-05-29T08:09:24.060304Z","shell.execute_reply.started":"2021-05-29T08:08:06.285925Z","shell.execute_reply":"2021-05-29T08:09:24.059028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In the case of VLAD feature, large dimensions lead to memory shortage, so dimensions are reduced using PCA\nif args_aggr==\"vlad\":\n    print(\"\\n\\nReduce dim of descriptor of the frames with PCA\")\n    start = time.time()\n    pca = PCA(n_components=pca_vlad, random_state=0)\n    pca.fit(train_global_desc)\n    train_global_desc = pca.transform(train_global_desc)\n    test_global_desc = pca.transform(test_global_desc)\n    print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:09:24.061882Z","iopub.execute_input":"2021-05-29T08:09:24.062275Z","iopub.status.idle":"2021-05-29T08:09:58.764066Z","shell.execute_reply.started":"2021-05-29T08:09:24.062232Z","shell.execute_reply":"2021-05-29T08:09:58.762952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nProcessing label\")\nstart = time.time()\n\n# For classification, label processing for each frame of train video for behavior classification\ntrain_global_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in train_global_desc_key])\ntrain_global_label = []\nfor fid in train_global_id:\n    cind = np.where(train_csv_arr[:, 0]==fid)[0]\n    clsname = train_csv_arr[cind, 1]\n    cinfo_ind = np.where(classinfo_arr[:, 1] == clsname)[0]\n    train_global_label.append(classinfo_arr[cinfo_ind, 0].astype(\"int\"))\ntrain_global_label = np.asarray(train_global_label).ravel()\n\n# For classification, processing ID for each frame of test video for behavior classification\ntest_global_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in test_global_desc_key])\n\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:09:58.765801Z","iopub.execute_input":"2021-05-29T08:09:58.766525Z","iopub.status.idle":"2021-05-29T08:09:59.7141Z","shell.execute_reply.started":"2021-05-29T08:09:58.766478Z","shell.execute_reply":"2021-05-29T08:09:59.713006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def saveFile(predict, predict_id, name, best_params=None):\n    #########################################################\n    ## Input \n    ##     predict : Behavior predictions for all test videos\n    ##     predict_id : id of all test videos\n    ##     name : Desired storage file name\n    ##     best_params : Use to save the desired instance\n    ##\n    #########################################################\n    \n    data = np.concatenate((np.expand_dims(predict_id.astype(\"str\"), axis=1), np.expand_dims(predict.astype(\"str\"), axis=1)), axis=1)\n    csv = pd.DataFrame(data, columns=['Id', 'Category'])\n    csv.to_csv(name + \".csv\", index=False)\n    \n    if best_params:\n        f = open(name + \".pickle\", \"wb\")\n        pickle.dump(best_params, f, 2)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:09:59.715317Z","iopub.execute_input":"2021-05-29T08:09:59.715642Z","iopub.status.idle":"2021-05-29T08:09:59.722243Z","shell.execute_reply.started":"2021-05-29T08:09:59.715614Z","shell.execute_reply":"2021-05-29T08:09:59.721196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create and classify video features by averaging the features obtained from all frames of the video","metadata":{}},{"cell_type":"code","source":"print(\"\\n\\nSVM global averaging in frame\")\nstart = time.time()\n\n\ntrain_video_feature = []\ntrain_video_label = []\ntest_video_feature = []\n\nfor i in range(0, train_global_id.shape[0], 5) :\n    # train_global_id의 수 만큼 반복하되, 비디오 프레임은 5개씩 존재하므로\n    # i를 5씩 증가시킵니다.\n    train_desc_mean = np.mean(train_global_desc[i:i+5], axis = 0)\n    # train_global_desc을 5개씩 평균내어 train_desc_mean에 평균값을 저장합니다.\n    train_video_feature.append(train_desc_mean)\n    # 저장한 평균값을 train_video_feature에 append 합니다.\n    train_video_label.append(train_global_label[i])\n    # train_global_label을 프레임 5개당 하나씩 append 합니다.\n    \ntrain_video_feature = np.array(train_video_feature)\n# train_video_feature를 numpy array로 저장합니다.\ntrain_video_label = np.array(train_video_label)\n# train_video_label을 numpy array로 저장합니다.\n\nfor i in range(0, test_global_id.shape[0], 5) :\n    # test_global_id의 수 만큼 반복하되, 비디오 프레임은 5개씩 존재하므로\n    # i를 5씩 증가시킵니다.\n    test_desc_mean = np.mean(test_global_desc[i:i+5], axis = 0)\n    # test_desc_mean을 5개씩 평균내어 train_desc_mean에 평균값을 저장합니다.\n    test_video_feature.append(test_desc_mean)\n    # 저장한 평균값을 train_video_feature에 append 합니다.\n    \ntest_video_feature = np.array(test_video_feature)\n# test_video_feature를 numpy array로 저장합니다.\n\nclf = SVC(random_state = 0, class_weight = 'balanced')\n# 모델을 선언하고 파라미터를 조정합니다.\nclf.fit(train_video_feature, train_video_label)\n# 모델에 train_video_feature과 train_video_label를 넣어 학습시킵니다.\nsvm_predict = clf.predict(test_video_feature)\n# 모델에 test_video_feature를 넣고 결과값을 예측하여 svm_predict에 저장합니다.\n\n# baseline(bow) : 0.20990\n# random_state = 0 // kaggle : 0.17227\n# random_state = 0, C = 1 // kaggle: 0.17227\n# random_state = 0, C = 10, 100 // kaggle: 0.16831\n# random_state = 0, class_weight = 'balanced' // kaggle: 0.20594\n\n# baseline(vlad) : 0.28118\n# random_state = 0, class_weight = 'balanced' // kaggle: 0.28118\n\nsaveFile(classinfo_arr[svm_predict][:,1], np.arange(len(test_list)), \"svm_global_averaging\")\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:21:03.443602Z","iopub.execute_input":"2021-05-29T08:21:03.444577Z","iopub.status.idle":"2021-05-29T08:21:04.240795Z","shell.execute_reply.started":"2021-05-29T08:21:03.444501Z","shell.execute_reply":"2021-05-29T08:21:04.239447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using a classifier as a feature from all frames of the video, select the most frequently predicted behavior","metadata":{}},{"cell_type":"code","source":"print(\"\\n\\nSVM global voting in frame\")\nstart = time.time()\n\nclf = SVC(random_state = 0, class_weight = 'balanced')\n# 모델을 선언하고 파라미터를 설정합니다.\nclf.fit(train_global_desc, train_global_label)\n# 모델에 train_global_desc과 train_global_label을 학습시킵니다.\ny_pred = clf.predict(test_global_desc)\n# 모델에 test_global_desc을 넣어 예측하고 y_pred에 저장합니다.\ny_pred = y_pred.reshape(-1,5)\n# y_pred를 알맞게 reshape 합니다.\n\nfind_mode = mode(y_pred, axis = 1)\n# mode 함수를 이용하여 프레임별 예측값 y_pred의 최빈값을 구합니다.\nsvm_predict = find_mode[0].reshape(-1,)\n# 최빈값을 해당 비디오의 행동예측 값으로 선정합니다.\n\n# baseline(bow) : 0.20990\n# random_state = 0, class_weight = 'balanced' // kaggle: 0.20990\n\n# baseline(vlad) : 0.28118\n# random_state = 0, class_weight = 'balanced' // kaggle: 0.26732\n\nsaveFile(classinfo_arr[svm_predict][:,1], np.arange(len(test_list)), \"svm_global_voting\")\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:10:01.070702Z","iopub.execute_input":"2021-05-29T08:10:01.071034Z","iopub.status.idle":"2021-05-29T08:10:25.053464Z","shell.execute_reply.started":"2021-05-29T08:10:01.071005Z","shell.execute_reply":"2021-05-29T08:10:25.052302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### After selecting the representative feature in the frame feature, describe the video feature using BoW or VLAD method","metadata":{}},{"cell_type":"code","source":"train_global_alloc, test_global_alloc, global_codebook, global_kmeans = clustering(train_global_desc, test_global_desc, args_global_cluster)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-29T08:10:25.055259Z","iopub.execute_input":"2021-05-29T08:10:25.055703Z","iopub.status.idle":"2021-05-29T08:10:26.820139Z","shell.execute_reply.started":"2021-05-29T08:10:25.05566Z","shell.execute_reply":"2021-05-29T08:10:26.819081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nAllocate center & Descript global histogram\")\nstart = time.time()\ntrain_vid_names = np.asarray([i.split(\", \")[0] for i in train_global_desc_key])\ntrain_vid_names_u = np.unique(train_vid_names)\n\n# Train 비디오 내 프레임 별로 기술된 이미지 feature를 기반으로 한번 더 기술하여(한번 더 BoW 혹은 VLAD)\n# 각 비디오에 대한 비디오 feature 기술\ntrain_video_desc = []\ntrain_video_desc_key = []\nfor vid_name in train_vid_names_u:\n    cind = np.where(vid_name==train_vid_names)[0]\n    if args_aggr==\"bow\":\n        hist_desc = BoW(train_global_alloc[cind], args_global_cluster)\n    elif args_aggr==\"vlad\":\n        hist_desc = VLAD(train_global_desc[cind], train_global_alloc[cind], global_codebook)\n    else:\n        import pdb; pdb.set_trace()\n\n    train_video_desc.append(hist_desc)\n    train_video_desc_key.append(vid_name)\ntrain_video_desc = np.asarray(train_video_desc)\ntrain_video_desc_key = np.asarray(train_video_desc_key)\n\n# Test 비디오 내 프레임 별로 기술된 이미지 feature를 기반으로 한번 더 기술하여(한번 더 BoW 혹은 VLAD)\n# 각 비디오에 대한 비디오 feature 기술\ntest_vid_names = np.asarray([i.split(\", \")[0] for i in test_global_desc_key])\ntest_vid_names_u = np.unique(test_vid_names)\n\ntest_video_desc = []\ntest_video_desc_key = []\nfor vid_name in test_vid_names_u:\n    cind = np.where(vid_name==test_vid_names)[0]\n    if args_aggr==\"bow\":\n        hist_desc = BoW(test_global_alloc[cind], args_global_cluster)\n    elif args_aggr==\"vlad\":\n        hist_desc = VLAD(test_global_desc[cind], test_global_alloc[cind], global_codebook)\n    else:\n        import pdb; pdb.set_trace()\n\n    test_video_desc.append(hist_desc)\n    test_video_desc_key.append(vid_name)\ntest_video_desc = np.asarray(test_video_desc)\ntest_video_desc_key = np.asarray(test_video_desc_key)\n\n\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:10:26.823696Z","iopub.execute_input":"2021-05-29T08:10:26.823985Z","iopub.status.idle":"2021-05-29T08:10:38.414566Z","shell.execute_reply.started":"2021-05-29T08:10:26.823956Z","shell.execute_reply":"2021-05-29T08:10:38.413523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nProcessing label\")\nstart = time.time()\n\n# 분류를 위해, 행동 분류에 대한 각 train 비디오 별 label 가공\ntrain_video_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in train_video_desc_key])\ntrain_video_label = []\nfor fid in train_video_id:\n    cind = np.where(train_csv_arr[:, 0]==fid)[0]\n    clsname = train_csv_arr[cind, 1]\n    cinfo_ind = np.where(classinfo_arr[:, 1] == clsname)[0]\n    train_video_label.append(classinfo_arr[cinfo_ind, 0].astype(\"int\"))\ntrain_video_label = np.asarray(train_video_label).ravel()\n\n# 분류를 위해, 행동 분류에 대한 각 test 비디오 별 id 가공\ntest_video_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in test_video_desc_key])\n\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:10:38.416138Z","iopub.execute_input":"2021-05-29T08:10:38.416807Z","iopub.status.idle":"2021-05-29T08:10:38.635497Z","shell.execute_reply.started":"2021-05-29T08:10:38.416764Z","shell.execute_reply":"2021-05-29T08:10:38.634407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 이미지 feature에 대해 다시 한번 VLAD feature 기술 방식을 사용하여 video feature를 기술한 경우 큰 차원으로 인해 메모리 부족 현상이 발생하므로 PCA를 이용한 차원 축소\nif args_aggr==\"vlad\":\n    print(\"\\n\\nReduce dim of descriptor of the frames with PCA\")\n    start = time.time()\n    pca = PCA(n_components=pca_vlad, random_state=0)\n    pca.fit(train_video_desc)\n    train_video_desc = pca.transform(train_video_desc)\n    test_video_desc = pca.transform(test_video_desc)\n    print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:10:38.636886Z","iopub.execute_input":"2021-05-29T08:10:38.637247Z","iopub.status.idle":"2021-05-29T08:10:46.589856Z","shell.execute_reply.started":"2021-05-29T08:10:38.637207Z","shell.execute_reply":"2021-05-29T08:10:46.588735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nSVM video descriptor\")\nstart = time.time()\n\nclf = SVC(random_state = 0)\nclf.fit(train_video_desc, train_video_label)\nsvm_predict = clf.predict(test_video_desc[test_video_id])\n\n# baseline(bow) : 0.20990\n# random_state = 0, class_weight = 'balanced' // kaggle: 0.07128\n\n# baseline(vlad) : 0.28118\n# random_state = 0, class_weight = 'balanced' // kaggle: 0.07524\n\nsaveFile(classinfo_arr[svm_predict][:,1], test_video_id, \"svm_video\")\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:18:43.324636Z","iopub.execute_input":"2021-05-29T08:18:43.32504Z","iopub.status.idle":"2021-05-29T08:18:44.596074Z","shell.execute_reply.started":"2021-05-29T08:18:43.324948Z","shell.execute_reply":"2021-05-29T08:18:44.59489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}