{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:27:59.886159Z","iopub.execute_input":"2021-06-05T08:27:59.886635Z","iopub.status.idle":"2021-06-05T08:27:59.899571Z","shell.execute_reply.started":"2021-06-05T08:27:59.886596Z","shell.execute_reply":"2021-06-05T08:27:59.898637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:27:59.901109Z","iopub.execute_input":"2021-06-05T08:27:59.901598Z","iopub.status.idle":"2021-06-05T08:27:59.908848Z","shell.execute_reply.started":"2021-06-05T08:27:59.901561Z","shell.execute_reply":"2021-06-05T08:27:59.907836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download punkt data from nltk library. This data is needed for the tokeninizer of the character.\n# Download stopwords using the nltk library.\n\nnltk.download('punkt')\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:27:59.910596Z","iopub.execute_input":"2021-06-05T08:27:59.911142Z","iopub.status.idle":"2021-06-05T08:27:59.932483Z","shell.execute_reply.started":"2021-06-05T08:27:59.911104Z","shell.execute_reply":"2021-06-05T08:27:59.931616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/2021-ml-tp2-spam/train.csv\",encoding=\"latin-1\")\ndf_test = pd.read_csv(\"/kaggle/input/2021-ml-tp2-spam/test.csv\",encoding=\"latin-1\")\ndf_train.Data","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:27:59.934008Z","iopub.execute_input":"2021-06-05T08:27:59.93449Z","iopub.status.idle":"2021-06-05T08:27:59.971951Z","shell.execute_reply.started":"2021-06-05T08:27:59.934457Z","shell.execute_reply":"2021-06-05T08:27:59.970907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = df_train[\"Data\"]\ny_train = df_train[\"Class\"]\nX_test = df_test[\"Data\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:27:59.973559Z","iopub.execute_input":"2021-06-05T08:27:59.973861Z","iopub.status.idle":"2021-06-05T08:27:59.97969Z","shell.execute_reply.started":"2021-06-05T08:27:59.973831Z","shell.execute_reply":"2021-06-05T08:27:59.978586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text data preprocessing","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer\ndef data_processing(text):\n    # If the input text is not an English character, it is changed to ' '.\n    pre_words = re.sub(pattern = '[^A-Za-z]', repl = ' ', string = text)\n    \n    # Convert uppercase letters to lowercase letters\n    pre_words = pre_words.lower()\n    \n    # Tokenize pre_word to list words and store them in tokenized_words\n    tokenized_words = word_tokenize(pre_words)\n    \n    # Find stopwords in English using the words() function of stopwords and store them in stops.\n    stops = nltk.corpus.stopwords.words('english')\n    \n    tokenized_words_remove = []\n    \n    for w in tokenized_words:\n        # If there is no stopword in tokenized_words w of tokenized word list, append to tokenized_words_remove list\n        if w not in stops:\n            tokenized_words_remove.append(w)\n    \n    stemmer = PorterStemmer()\n    for i in range(len(tokenized_words_remove)):\n        # Using the stem built-in function in PorterStemmer\n        # it goes through the process of changing the word with the same meaning to the same word in the tokenized_words_remove list.\n        tokenized_words_remove[i] = stemmer.stem(tokenized_words_remove[i])\n    \n    return ( \" \".join( tokenized_words_remove ))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:27:59.981492Z","iopub.execute_input":"2021-06-05T08:27:59.981835Z","iopub.status.idle":"2021-06-05T08:28:00.000217Z","shell.execute_reply.started":"2021-06-05T08:27:59.981791Z","shell.execute_reply":"2021-06-05T08:27:59.999146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=[data_processing(i) for i in X_train]\nX_test=[data_processing(i) for i in X_test]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:28:00.00742Z","iopub.execute_input":"2021-06-05T08:28:00.008045Z","iopub.status.idle":"2021-06-05T08:28:03.647711Z","shell.execute_reply.started":"2021-06-05T08:28:00.007992Z","shell.execute_reply":"2021-06-05T08:28:03.646858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bag of Word ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features = 100) # Define CountVectorizer or TfidfVectorizer\n# The parameter 'max_features = 100' means to extract as many as 100 corpus in the order of the highest frequency.\n\nX_train = np.array(X_train, dtype = 'U')\nX_test = np.array(X_test, dtype = 'U')\n\nX_train_features = vectorizer.fit_transform(X_train)\nX_test_features = vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:28:03.649096Z","iopub.execute_input":"2021-06-05T08:28:03.649368Z","iopub.status.idle":"2021-06-05T08:28:03.782627Z","shell.execute_reply.started":"2021-06-05T08:28:03.649341Z","shell.execute_reply":"2021-06-05T08:28:03.781645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train[y_train==\"ham\"] = 0\ny_train[y_train==\"spam\"] = 1\ny_train = y_train.astype(\"uint8\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:28:03.784791Z","iopub.execute_input":"2021-06-05T08:28:03.785234Z","iopub.status.idle":"2021-06-05T08:28:03.799865Z","shell.execute_reply.started":"2021-06-05T08:28:03.785187Z","shell.execute_reply":"2021-06-05T08:28:03.799126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM: classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC(gamma = 'auto', C = 100) # Setting the parameters\n\n# Parameter setting history\n# gamma = 'auto' // kaggle : 0.94439\n# gamma = 'auto', C = 5 // kaggle : 0.95695\n# gamma = 'auto', C = 10 // kaggle : 0.95784\n# gamma = 'auto', C = 100 // kaggle : 0.96322 **\n# gamma = 'auto', C = 1000 // kaggle : 0.96143\n# gamma = 'auto', C = 100, kernel = 'linear'// kaggle: 0.95874\n# MaxAbsScaler(), gamma = 'auto', C = 100 // kaggle: 0.95695\n# gamma = 'auto', C = 100, kernel = 'sigmoid' // kaggle: 0.96053\n# gamma = 'auto', C = 10, kernel = 'sigmoid' // kaggle: 0.95784\n# gamma = 'auto'//0.96233\n# gamma = 'auto', class_weight = 'balanced' //0.95605\n\n\nsvc.fit(X_train_features, y_train)\ny_pred = svc.predict(X_test_features)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:28:03.802902Z","iopub.execute_input":"2021-06-05T08:28:03.803618Z","iopub.status.idle":"2021-06-05T08:28:04.003244Z","shell.execute_reply.started":"2021-06-05T08:28:03.803567Z","shell.execute_reply":"2021-06-05T08:28:04.002271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict to CSV","metadata":{}},{"cell_type":"code","source":"df_pred={\"ID\": range(np.array(y_pred).shape[0]),\"Class\":y_pred}\ndf_pred=pd.DataFrame(df_pred)\ndf_pred.to_csv(\"predict.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:28:04.004999Z","iopub.execute_input":"2021-06-05T08:28:04.005894Z","iopub.status.idle":"2021-06-05T08:28:04.016236Z","shell.execute_reply.started":"2021-06-05T08:28:04.005824Z","shell.execute_reply":"2021-06-05T08:28:04.015374Z"},"trusted":true},"execution_count":null,"outputs":[]}]}